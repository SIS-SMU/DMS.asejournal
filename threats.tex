As we have noticed, the improvement of \textsc{Dms} in single-fault and multi-fault programs are different. In the 12 single-fault programs,  \textsc{Dms} requires the minimal amount of labeling effort by achieving 67.7\% labeling reduction on Unix programs and 10\% reduction on Siemens programs in comparison with the existing best approach---{\sc Raptor}. While in the 8 multi-fault programs, \textsc{Dms} achieves 5.95\% labeling reduction in comparison with the existing best approach---{\sc Fep-Addtl}. The phenomenon happens since we consider the worst cases of \textsc{Dms} in multi-fault programs, i.e., we consider the root case $d_*$ with the lowest suspiciousness score. In some versions of the multi-fault programs, \textsc{Dms} needs more test cases to achieve 101\% of base line effectiveness than that of {\sc Fep-Addtl}. For example, in the version 2 of the program print\_token2, \textsc{Dms} need to label 500 test cases to achieve 101\% of base line effectiveness, while {\sc Fep-Addtl} only requires 59 test cases. Thus, the average reduced number of test cases for \textsc{Dms} in multi-fault program is not as high as that in single-fault programs.

However, the improvements of \textsc{Dms} in reducing cost are statistically significant for both single-fault and multi-fault programs at 95\% confidence interval via paired Wilcoxon signed-rank tests.
%show that \textsc{Dms} is statistically significantly better than the existing best approach on the Unix programs of single-fault subject programs, and multi-fault subject programs at 95\% confidence interval. 
Moreover, although we notice that \textsc{Dms} reduce the number of test case of Fep-Addtl by 5.95\% in multi-fault programs, but there are 51.45\% fault versions that \textsc{Dms} show positive improvement over Fep-Addtl, and  29.48\% fault versions that \textsc{Dms} show negative deterioration over Fep-Addtl, while the number for single-fault programs are 40.16\% and 20.08\%.


The threats to our studies include the issue of how
representative the subjects of our studies are. Since the Siemens programs
are small, and larger programs may be subject to different testing and debugging traits.
To strengthen the external validity, we include \textsc{Unix} programs which are
real-life programs. These subjects have been adopted for evaluation in many
studies \citep[e.g.][]{JH05,Abreu:2009.jss,DBLP:conf/icse/SantelicesJYH09}.

Another possible threat is that although our method outperforms existing method
in 25.2\% to 62.99\% program versions and gets equivalent cost in around 30\% versions,
there are still a certain percent of versions that our method does not perform very well.
But as we can see in the studies, most of the negative deterioration of those versions
are relatively small
% or even trivial 
comparing to the positive improvements. We also
conduct statistical tests to further confirm the advantage of \textsc{Dms}.

%At last, the studies show that \textsc{Dms} outperforms other methods
%in reducing manual effort for debugging, but the ultimate
%merit should be evaluated by end-users. However, due to the
%expense as well as difficulty of user study, most existing works in
%test case prioritization and fault localization are temporarily evaluated
%by authors~\cite{RUCH01,SEAGMGR01,LHH07,Abreu:2009.jss}.

There are also many other kinds of threats to validity affecting fault localization techniques in general as listed in a recent study by \cite{Steimann2013}, such as heterogeneity of test cases, biases in injected faults, unrealistic assumptions about locating and understanding faults, etc. Although we focus on evaluating test case prioritization techniques, instead of fault localization techniques, our work inevitably inherits the threats to validity for fault localization techniques since our evaluation of prioritization techniques is done through the evaluation of fault localization. We hope in future work the threats to validity for both fault localization and test case prioritization techniques can be addressed together.
