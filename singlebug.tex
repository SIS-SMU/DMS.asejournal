Section~\ref{sec.exp.setup} gives details about our experimental setup for single-fault programs.
In Section~\ref{sec.exp.subject}, we introduce the subject programs used in our study. Sections~\ref{sec.exp.resultsA}~\&~\ref{sec.exp.resultsB} show the results.

\subsubsection{Experimental Setups and Measures}\label{sec.exp.setup}

In our experiment, every test case prioritization technique starts from
an arbitrary labeled failed trace because developers start debugging only when
test cases fail.

In this paper, we use \textsc{Raptor} as the bootstrapping technique ($\mathcal{P}$ in Figure \ref{algo:DMS}). During the bootstrapping process, $w$ is set to 10 to facilitate trend analysis. 

Following \cite{JiangCT11}, for each faulty version, we repeat each prioritization technique 20 times to obtain its average cost. For each time, a randomly chosen failed trace is used as the starting point to alleviate the sensitivity of the technique to the choice of starting traces. On the other hand, to fairly compare our approach with other prioritization methods, the {\em same randomly} chosen failed traces are used as the starting traces for all methods.

The effectiveness of test case prioritization methods would be manifested as the effectiveness of the subsequent fault localization results. 
So one way to compare the effectiveness of different prioritization methods based on the different diagnostic costs of the subsequently applied fault localization technique when the same number of test cases are selected by the different prioritization methods. In the literature, many fault localization studies use the percentage of program elements that need to be inspected according to the ranked list of fault localization results to locate all faults as one kind of diagnostic costs,
which is defined as follows:

\begin{equation}\label{equation.avgcost}
	cost = \dfrac{\left|  \ \{ j \ | \ f_{T_{\mathcal{S}}}(d_j) \geq f_{T_{\mathcal{S}}}(d_*) \}  \ \right|}{\left|  \mathcal{D} \right|}
\end{equation}
where $\mathcal{D}$ consists of all program elements appearing in the program and $d_*$ represents the fault(s) (i.e., the root cause(s) of failures) of a program.
We calculate the average cost as the percentage of elements that developers have
to examine until locating the root causes ($d_*$) of failures. The lower the cost is, the better a fault localization technique is. 
Since multiple
program elements can be assigned with the same suspicious score, the numerator
is considered as the number of program elements $d_j$ that have bigger or the
same suspicious score to $d_*$ in this paper.\footnote{There are a number of alternative ways to define diagnostic costs and accuracies, such as using the number of faults located when up to a certain percentage of program elements are inspected \cite[e.g.][]{Wong2014,Debroy2013,CZ05,BaahPH10,JHS02,Lucia2014}, or assuming a random ordering for elements with the same score and incorporating their expected rank $\frac{\left| \{ j \ | \ f_{T_{\mathcal{S}}}(d_j) = f_{T_{\mathcal{S}}}(d_*) \} \right| + 1}{2}$ in calculating cost \cite[e.g.][]{Ali2009,Steimann2013,Steimann2012,Xu2011}. We use the one in Equation \ref{equation.avgcost} and \ref{equation.accuracy} as they are commonly used and easy to understand, and our focus is on evaluating whether different test case prioritization techniques {\em change} the diagnostic costs, instead of measuring the absolute costs. We believe that if our technique shows significant improvements over one kind of diagnostic cost, it should also show improvements over other kinds of diagnostic cost.}

We can also define the {\em accuracy} of a fault localization technique as the reverse of the cost, which is the higher the better:

\begin{equation}\label{equation.accuracy}
	accuracy = 1 - cost
\end{equation}
In the following parts of the paper, we thus use {\em cost} and {\em accuracy} interchangeably when the context is clear.

Another way to measure the effectiveness of test case prioritization methods is to see how many test cases can be reduced by each method.
A major goal of our paper is to minimize the number of test cases that need manual labelling but can maintain fault localization accuracies. So, in the following evaluation results, we also show the numbers of reduced test cases {\em with respect to a targeted fault localization cost (or accuracy)}.

If labeling all test cases and performing fault localization on all program spectra results in an average diagnostic cost $c$, we call it the {\em base line cost}. If a test prioritization technique or fault localization technique leads to a diagnostic cost $c'$, then we say the technique achieves {\em $x$\% of base line effectiveness}, where $x$ is defined as follows:

\begin{equation}\label{equation.baselinecost}
	x = \frac{c'}{c} \times 100
\end{equation}

To be fair, the number of reduced test cases by each prioritization technique should be measured when the technique achieves 100\% of base line effectiveness. However, in reality, it is hardly possible to directly control the cost to be exactly 100\% of base line. So, we allow 1\% deviation; i.e., in the following evaluation results, we measure the numbers of reduced test cases {\em when at most 101\% of base line effectiveness is achieved}.


\subsubsection{Subject Programs}\label{sec.exp.subject}

We use five real {\em C} programs and seven Siemens test programs from the
{\em Software-artifact Infrastructure Repository}~(SIR)~\citep{doESE05}. We refer to the five real programs (\texttt{sed}, \texttt{flex}, \texttt{grep}, \texttt{gzip}, and \texttt{space}) as \textsc{Unix} programs. Table \ref{dataset} shows the descriptive statistics of each subject,
including the number of faults, available test cases and code sizes. Following many previous studies \citep[e.g.][]{JHS02,Abreu:2009.jss}, we exclude faults not directly observable by the {\tt gcov} profiling
tool\footnote{http://gcc.gnu.org/onlinedocs/gcc/Gcov.html} (e.g., some faults lead to a crash before \texttt{gcov} dumps profiling information and some faults do not cause any test case to fail), and in total we study 254 faults.

\begin{table}[!htbp]
	%\vspace{-8pt}
	\centering
	\caption{Subject Programs}
	\renewcommand{\arraystretch}{1.5}
	\small
    \begin{tabular}{|l|c|c|c|c|} \hline
        Program & Description & LOC  & Tests & Faults\\ \hline\hline
		tcas & Aircraft Control & 173 & 1609 & 41\\ \hline
        schedule2 & Priority Scheduler & 374  & 2710  & 8\\ \hline
        schedule & Priority Scheduler & 412 & 2651  & 8\\ \hline
        replace & Pattern Matcher & 564 & 5543  & 31\\ \hline
		tot\_info & Info Measure & 565 & 1052  & 22\\ \hline
        print\_tokens2 & Lexical Analyzer & 570  & 4055  & 10\\ \hline
        print\_tokens & Lexical Analyzer & 726 & 4070  & 7\\ \hline
        space & ADL Compiler & 9564 & 1343 & 30\\ \hline
        flex & Lexical Parser & 10124 & 567  & 43\\ \hline
        sed & Text Processor & 9289  & 371  & 22\\ \hline
        grep & Text Processor & 9089 & 809  & 17\\ \hline
        gzip & Data Compressor & 5159 & 217  & 15\\ \hline
	\end{tabular}
	\label{dataset}
\end{table}

\subsubsection{Experimental Results: Reducing Number of Test Cases}\label{sec.exp.resultsA}
%In this subsection, we conduct several controlled experiments to show the effectiveness of \textsc{Dms}.

%\vspace{-4pt}
%\subsubsubsection{Effectiveness on Reducing The Number of Test Cases Needed for a Target Cost}

Here, we investigate the effectiveness of \textsc{Dms} in reducing the number of test cases needed for a targeted diagnostic cost. We compare \textsc{Dms} with previous test case prioritization techniques in terms of labeling effort when to achieve 101\% of base line effectiveness as stated in Section \ref{sec.exp.setup}.
%\lx{moved to setup section to explain our metrics.}
%given an expected fault localization accuracy.
%Since the major objective of our solution is to save labelling effort at the same time retain acceptable fault localization accuracy.
%If labeling all test cases and performing fault localization on all program spectra results in an average diagnostic cost $c$, we call it the base line cost. Then we define $x$\% base line effectiveness ($c_x$) as follows:

%\begin{equation}\label{equation.accu_cost}
%	c_{x} = \frac{x}{100} \times c
%\end{equation}

Since Dms would output a ranked list of suspicious program elements, we compute the diagnostic cost $c_n$ for {\sc Dms} when we just inspect top $n$ ($n \in \{1,2,\cdot \cdot \cdot, |D|\}$) suspicious elements. We record the {\em maximum} $n$ that $c_n$ is still within 101\% of base line cost as the amount of labeling effort. Also, we limit the maximum number of test cases allowed to select (i.e., $k$ in Algorithm \ref{algo:DMS}) to 500 in this specific evaluation.

Table \ref{tab:label_effort} shows how many labels are needed on average to achieve 101\% of base line effectiveness
% (i.e., within 1\% accuracy lost) 
for each approach.
E.g., \textsc{Raptor} requires 48 labels on average for each faulty version from the 5 \textsc{Unix} programs while \textsc{Dms} only needs 16.
Overall, \textsc{Dms} requires the minimal amount of labeling effort by achieving 67.7\% labeling reduction on \textsc{Unix}
programs and 10\% reduction on Siemens programs in comparison with the existing best approach (\textsc{Raptor}).

\begin{table}[tbp]
	%\vspace{-8pt}
	\centering
	\caption{Labeling Effort on Subject Programs}
{
	\scriptsize
		\renewcommand{\arraystretch}{1.5}
%		\hspace{-10pt}
        \begin{tabular}{|m{32pt}|m{13pt}|c|c|m{21pt}|m{21pt}|m{21pt}|m{17pt}|}
		   \hline
		     Subject &             &                &                  & \textsc{Stmt-} & \textsc{Stmt-} & \textsc{Fep-}  & \textsc{Art-} \\
		   Programs & \textsc{Dms} & \textsc{Raptor}  & \textsc{Sequoia} & \textsc{Addtl} & \textsc{Total} & \textsc{Addtl} & \textsc{Min} \\
		   \hline\hline
		   Siemens &   {\bf 18} &         20 &       500\tiny{+} &       500\tiny{+} &       500\tiny{+} &         97 &        150 \\
		   \hline
			\textsc{Unix} &   {\bf 16} &         48 &    176 &        150 &       500\tiny{+} &         98 &         56 \\
		   \hline
		\end{tabular}
}
	\label{tab:label_effort}
\end{table}


%\vspace{-6pt}
\subsubsection{Experimental Results: Reducing Cost}\label{sec.exp.resultsB}
%\subsubsubsection{Effectiveness on Reducing Cost for a Given Number of Labeled Test Cases}

Here, we investigate the effectiveness of \textsc{Dms} in reducing cost given a targeted number of labeled test cases. We select 30 test cases (i.e., set $k=30$ in Algorithm \ref{algo:DMS}), which we believe are not too many to manually label. We also find that in our experiments the average debugging cost of using \textsc{Dms} will not reduce noticeably even if more labeled test cases beyond 50 are added further (See Figure \ref{Dms_boxplot}), which is in line with studies in the literature \cite[e.g.][]{Abreu:2009.jss,Libl+05} that tens of passed and failed spectra may suffice for fault localization. During the bootstrapping process, the first 10 test cases are picked by \textsc{Raptor}. We use different prioritization techniques and apply {\em Ochiai} to evaluate program elements on the selected program spectra. A prioritization technique that obtains a lower cost is better.

\begin{figure}[!htbp]
    \centering
    %\hspace{-0.4cm}
    \includegraphics[width=12cm]{sdm_boxplot.pdf}
   % \vspace*{-12pt}
    \caption{Average Cost of {\sc Dms} when Selecting Different Numbers of Test Cases.}
    \label{Dms_boxplot}
\end{figure}

Following \cite{BaahPH10,BaahPH11} and the {\em cost}
metric (Equation~\ref{equation.avgcost}), we compare
the effectiveness of two prioritization methods $P_A$ and $P_B$ by using
one of the methods (for example, $P_B$) as reference measure.
When selecting the same number of traces $k$, the cost difference:
$cost(P_B) - cost(P_A)$ is considered as the improvement of $P_A$
over $P_B$. A positive value means that $P_A$ performs better than
$P_B$ (since lower cost is better) and a negative value means that the performance deteriorates if we use $P_A$ to replace $P_B$.
The difference corresponds
to the magnitude of improvement. For example, if the cost of
test cases from $P_A$ is 30\% and the cost of $P_B$ is 40\%,
then the improvement of $P_A$ over $P_B$ is 10\%, which means
that developers would examine 10\% fewer statements if $P_A$
is deployed.

\vspace{0.2cm}
\noindent{\bf Result Summary.} Table \ref{tab:compare_11}, \ref{tab:compare_21}, and \ref{tab:compare_31} compare
our method with the existing prioritizing techniques. The results show that
our method outperforms no worse than other methods for the majority of faulty program versions.

%\vspace{-0.1cm}
\begin{table}[tbp]
%	\vspace{-8pt}
    \centering
		\caption{Comparison of Prioritization methods.}
		\renewcommand{\arraystretch}{1.5}
		\small
        \begin{tabular}{|c|c|c|c|}
			\hline
			Test Prioritization Method  &  Positive  &  Negative  &   Neutral  \\
			\hline\hline
			\textsc{Dms} vs \textsc{Raptor} & {\bf 25.20\%} &    19.29\% &    55.51\% \\
			\hline
			\textsc{Dms} vs \textsc{Sequoia} & {\bf 33.46\%} &    19.69\% &    46.85\% \\
			\hline
			\textsc{Dms} vs \textsc{Stmt-Addtl} & {\bf 42.13\%} &    19.29\% &    38.58\% \\
			\hline
			\textsc{Dms} vs \textsc{Stmt-Total} & {\bf 62.99\%} &     7.87\% &    29.13\% \\
			\hline
			\textsc{Dms} vs \textsc{Fep-Addtl} & {\bf 40.16\%} &    20.08\% &    39.76\% \\
			\hline
			\textsc{Dms} vs \textsc{Art-Min} & {\bf 31.50\%} &    19.29\% &    49.21\% \\
			\hline
		\end{tabular}
    \label{tab:compare_11}
\end{table}

%\vspace{-0.1cm}
As illustrated in Table \ref{tab:compare_11}, \textsc{Dms} performs
better than \textsc{Raptor} on 25.20\% of the faulty versions, worse
on 19.29\% of the faulty versions, and shows no improvement
on 55.51\% of the faulty versions. The first row of
Table \ref{tab:compare_21} characterizes the degree of positive improvement of
\textsc{Dms} over \textsc{Raptor}. As the table indicates, half of the 25.20\%
faulty versions with positive improvement values have improvements
between 0.03\% and 3.93\%, and the other half
have improvements between 3.93\% and 77.42\%. The average
positive improvement of \textsc{Dms} over \textsc{Raptor} is 7.71\%.

%\vspace{-0.1cm}
\begin{table}[tbp]
    \centering
		\caption{Distribution of positive improvements.}
		\renewcommand{\arraystretch}{1.5}
		\small
        \begin{tabular}{|c|c|c|c|c|}
			\hline
			Test Pri. Tech.  &        Max &       Mean &     Median &        Min \\
			\hline\hline
			\textsc{Dms} vs \textsc{Raptor} & {\bf 77.42\%} &     7.71\% &     3.93\% &     0.03\% \\
			\hline
			\textsc{Dms} vs \textsc{Sequoia} & {\bf 66.67\%} &    14.38\% &     8.06\% &     0.23\% \\
			\hline
			\textsc{Dms} vs \textsc{Stmt-Addtl} & {\bf 72.87\%} &    14.68\% &     5.17\% &     0.03\% \\
			\hline
			\textsc{Dms} vs \textsc{Stmt-Total} & {\bf 94.97\%} &    27.68\% &    22.29\% &     0.03\% \\
			\hline
			\textsc{Dms} vs \textsc{Fep-Addtl} & {\bf 45.90\%} &    13.83\% &     6.35\% &     0.03\% \\
			\hline
			\textsc{Dms} vs \textsc{Art-Min} & {\bf 53.81\%} &     7.70\% &     3.23\% &     0.03\% \\
			\hline
		\end{tabular}
    \label{tab:compare_21}
\end{table}

Table \ref{tab:compare_31} characterizes the degree of negative deterioration of
\textsc{Dms} over other techniques. As the first row in the table indicates, for half of the 19.29\%
faulty versions, {\sc Dms} deteriorates 
% with positive improvement values have improvements
between 0.03\% and 0.60\% from \textsc{Raptor}, and for the other half,
%have improvements 
{\sc Dms} deteriorates between 0.60\% and 1.15\%. The average
percentage of negative deterioration of \textsc{Dms} over \textsc{Raptor} is 0.54\%.

\begin{table}[tbp]
    \centering
		\caption{Distribution of negative deterioration.}
		\renewcommand{\arraystretch}{1.5}
		\small
        \begin{tabular}{|c|c|c|c|c|}
			\hline
			Test Pri. Tech.  &        Max &       Mean &     Median &        Min \\
			\hline\hline
			\textsc{Dms} vs \textsc{Raptor} & {\bf 1.15\%} &     0.54\% &     0.60\% &     0.03\% \\
			\hline
			\textsc{Dms} vs \textsc{Sequoia} & {\bf 31.71\%} &    4.01\% &     1.33\% &     0.03\% \\
			\hline
			\textsc{Dms} vs \textsc{Stmt-Addtl} & {\bf 30.73\%} &    4.14\% &     1.52\% &     0.03\% \\
			\hline
			\textsc{Dms} vs \textsc{Stmt-Total} & {\bf 27.88\%} &    4.61\% &    2.64\% &     0.17\% \\
			\hline
			\textsc{Dms} vs \textsc{Fep-Addtl} & {\bf 24.70\%} &    5.06\% &     2.15\% &     0.03\% \\
			\hline
			\textsc{Dms} vs \textsc{Art-Min} & {\bf 22.41\%} &     4.11\% &     1.72\% &     0.03\% \\
			\hline
		\end{tabular}
    \label{tab:compare_31}
\end{table}


We conduct paired Wilcoxon signed-rank test to confirm the difference in performance between \textsc{Dms} and six existing prioritization techniques.
The statistical test result rejects the null hypothesis and suggests that the improvements of \textsc{Dms} over other existing techniques 
are statistically significant
%better than the existing best approach on \textsc{Unix} programs 
at 95\% confidence interval.

\vspace{0.2cm}
\noindent{\bf Detailed Comparison.} Table \ref{tab:label_effort} shows that \textsc{Raptor}, \textsc{Fep-Addtl} and \textsc{Art-Min} achieve 101\% of base line effectiveness with less than 500 test cases on subject programs.
Figure~\ref{fig:our_vs_fep}, \ref{fig:our_vs_artmin}, and~\ref{fig:our_vs_ag_unix}
show the comparison of fault localization costs between {\sc Dms} and the three different
prioritization techniques.
The horizontal axes represent the number of versions that
show differences in the Cost of
fault localization. The vertical axes represent the percentage
difference in Costs. If \textsc{Dms} is better than the reference
method, the area above zero-level line will be larger.

\vspace{0.2cm}
\noindent{\it D{\scriptsize MS} vs F{\scriptsize EP}-A{\scriptsize DDTL}.}
Previous studies~\citep{RUCH01,SEAGMGR01} show that \textsc{Fep-Addtl} is the most
promising prioritizing method for fault detection.
%However \textsc{Fep-Addtl} is not suitable for diagnostic prioritization,
%since it is initially proposed for regression testing, which assumes that
%tester have already get test oracles for each test case. But measuring
%{\em FEP} requires test oracles for all test cases which are absent in our problem.
%As a result, without test oracles {\em FEP} cannot evaluate the fault
%detection rate of each test case on program mutants.
%To circumvent this problem, Alberto \etal~\citep{Gonzalez-SanchezPAGG11} approximated {\em FEP}
Without test oracles, \textsc{Fep} can be estimated by $1 - ${\em False Negative Rate} (\textsc{Fnr})~\citep{Gonzalez-SanchezPAGG11}
\footnote{\textsc{Fnr} is the program passing rate when program element is the real fault and executed in test case. Usually when
\textsc{Fnr} is high, the fault is difficult to be detected by Spectrum-based
fault localization techniques.} which is also used in our study.
Figure \ref{fig:our_vs_fep} presents the comparison
between \textsc{Dms} and \textsc{Fep-Addtl} over all faulty versions that show performance differences.
\textsc{Fep-Addtl} is used as the reference prioritization technique.
The baseline represents the fault localization cost on program spectra prioritized
by \textsc{Fep-Addtl}. Each program version is a bar in this graph and we remove
versions from the graph that have no cost differences.
In the figure, the vertical axis represents the magnitude of
improvement of \textsc{Dms} over \textsc{Fep-Addtl}.
If the bar of a faulty version is above the
horizontal axis, that means on this version \textsc{Dms} performs
better than \textsc{Fep-Addtl} (positive improvement) and the bars
below the horizontal axis represent faulty versions for which
\textsc{Dms} performs worse than \textsc{Fep-Addtl}.

\begin{figure}[tbp]
    \centering
    \includegraphics[width=12cm]{our_vs_fep.pdf}
%    \vspace{-0.3cm}
    \caption{Improvement of D{\scriptsize MS} over F{\scriptsize EP}-A{\scriptsize DDTL}.}
    \label{fig:our_vs_fep}
\end{figure}
%\vspace{0.2cm}

The comparison shows that \textsc{Dms} performs better than \textsc{Fep-Addtl} 
%for the majority of faulty versions.
%, our prioritization method performs
%better than \textsc{Fep-Addtl} 
on 102 versions, out of 153 versions that show differences in cost, but performs worse than
\textsc{Fep-Addtl} on 51 versions.
The positive improvement ranges from 0.03\% to 45.90\%, with an average of 6.35\%.


\begin{figure}[tbp]
    \centering
    \includegraphics[width=12cm]{our_vs_artmin.pdf}
%    \vspace{-0.3cm}
    \caption{Improvement of D{\scriptsize MS} over A{\scriptsize rt}-M{\scriptsize IN}.}
    \label{fig:our_vs_artmin}
\end{figure}

\vspace{0.2cm}
\noindent{\it D{\scriptsize MS} vs A{\scriptsize rt}-M{\scriptsize IN}.} In this study we compare the effectiveness of \textsc{Dms} to {\em Adaptive Random Test Prioritization}(\textsc{Art})~\citep{JiangZCT09}.
There are various strategies for \textsc{Art}, in this experiment we only compare with the best one: \textsc{Art-Min}~\citep{JiangZCT09, Gonzalez-SanchezPAGG11, Alberto2011}.
Figure \ref{fig:our_vs_artmin} shows the results of the study in which \textsc{Art-Min} is used
as the baseline method. The comparison shows that \textsc{Dms} is better than \textsc{Art-Min}.
Out of 129 versions that show differences in cost, our prioritization method performs
better than \textsc{Art-Min} on 80 versions but performs worse than the
\textsc{Art-Min} on 49 versions.
%The positive improvement ranges from 0.03\% to 53.81\%, with an average of 7.70\%.

\vspace{0.2cm}
\noindent{\it D{\scriptsize MS} vs R{\scriptsize APTOR}.}
Figure \ref{fig:our_vs_ag_unix} shows the comparison between \textsc{Dms} and \textsc{Raptor} on \textsc{Unix} programs.
Here we use \textsc{Raptor} as the reference metric. The comparison shows that \textsc{Dms} 
%is better than \textsc{Raptor}.
%On \textsc{Unix} programs, 
outperforms \textsc{Raptor} on 20 versions by at least 1\% cost,
and on only 5 versions, it is worse than {\sc Raptor} by over 1\% cost.

\begin{figure}[tbp]
    \centering
    \includegraphics[width=12cm]{our_vs_ag_unix.pdf}
%    \vspace{-0.3cm}
    \caption{Improvement of D{\scriptsize MS} over R{\scriptsize APTOR} on U{\scriptsize NIX} programs.}
    \label{fig:our_vs_ag_unix}
\end{figure}

There is also improvement on Siemens programs: 32.2\% versions show differences and the average debugging cost improvement is 1.3\%, which is not so significant as compared with \textsc{Unix} programs.
This is probably due to the small software size. On Siemens programs, {\sc Raptor}
can reach 101\% of base line effectiveness by only selecting 20 test cases on average (see Table \ref{tab:label_effort}).
By selecting such few test cases, \textsc{Raptor} already obtains the maximal ambiguity group reduction due to very limited different
coverage profiles. For example, all test cases of \texttt{tcas} only have less than 15 ambiguity groups in all faulty versions. In this case,
the speedup by our method is trivial. In real scenario, programs to be diagnosed would be more similar to \textsc{Unix} programs.
